{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _rmse (x, x_hat):\n",
    "    x = x[1:]\n",
    "    x_hat = x_hat[1:]\n",
    "    return np.sqrt(np.mean(((x-x_hat)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Sets in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Path to the Data'\n",
    "with open(path+'train_resampled.pickle', 'rb') as handle:\n",
    "    train_resampled = pickle.load(handle)\n",
    "with open(path+'test_resampled.pickle', 'rb') as handle:\n",
    "    test_resampled = pickle.load(handle)\n",
    "with open(path+'dis_train.pickle', 'rb') as handle:\n",
    "    dis_train = pickle.load(handle)\n",
    "with open(path+'dis_test.pickle', 'rb') as handle:\n",
    "    dis_test = pickle.load(handle)\n",
    "with open(path+'Scalar.pickle', 'rb') as handle:\n",
    "    Scalar = pickle.load(handle)\n",
    "train_resampled = train_resampled[:13400]\n",
    "dis_train = dis_train[:13400]\n",
    "train_resampled = [train_resampled[idx][:-1].values for idx in range(len(train_resampled))]\n",
    "test_resampled = [test_resampled[idx].values[:-1] for idx in range(len(test_resampled))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Recurrent Neural Network Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, bias = True ):\n",
    "        \n",
    "        super(VRNN, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #feature-extracting transformations\n",
    "        self.phi_x = nn.Sequential( nn.Linear(self.x_dim , self.h_dim), nn.Tanh(), \n",
    "                                   nn.Linear(self.h_dim, self.h_dim), nn.Tanh())\n",
    "        self.phi_z = nn.Sequential( nn.Linear(self.z_dim, self.h_dim), nn.Tanh())\n",
    "\n",
    "        #encoder\n",
    "        self.enc = nn.Sequential( nn.Linear(self.h_dim + self.h_dim * self.n_layers , \n",
    "                                            self.h_dim), nn.Tanh(), nn.Linear(self.h_dim, self.h_dim), nn.Tanh())\n",
    "        \n",
    "        self.enc_mean = nn.Sequential( nn.Linear(self.h_dim, self.z_dim), nn.Tanh())\n",
    "        self.enc_std = nn.Sequential( nn.Linear(self.h_dim, self.z_dim), nn.Sigmoid())\n",
    "\n",
    "        #prior\n",
    "        self.prior = nn.Sequential( nn.Linear(self.n_layers * self.h_dim, self.h_dim), nn.Tanh())\n",
    "        self.prior_mean = nn.Sequential( nn.Linear(self.h_dim, self.z_dim), nn.Tanh())\n",
    "        self.prior_std = nn.Sequential( nn.Linear(self.h_dim, self.z_dim), nn.Sigmoid())\n",
    "\n",
    "        #decoder\n",
    "        self.dec = nn.Sequential( nn.Linear(self.h_dim + self.h_dim * self.n_layers , self.h_dim), nn.Tanh(), \n",
    "                                 nn.Linear(self.h_dim, self.h_dim), nn.Tanh())\n",
    "        self.dec_std = nn.Sequential( nn.Linear(self.h_dim, self.x_dim), nn.Sigmoid())\n",
    "        #self.dec_mean = nn.Linear(h_dim, x_dim)\n",
    "        self.dec_mean = nn.Sequential( nn.Linear( self.h_dim, self.x_dim ), nn.Tanh())\n",
    "\n",
    "        #recurrence\n",
    "        self.rnn = nn.GRU ( self.h_dim + self.h_dim + self.n_layers * self.h_dim, self.h_dim, \n",
    "                           self.n_layers, bias , batch_first = True )\n",
    "        \n",
    "    def encode (self, phi_x_t , h):\n",
    "        phi_x_t = self._concatenate (phi_x_t , h)\n",
    "        enc_t = self.enc(phi_x_t)\n",
    "        enc_mean_t = self.enc_mean(enc_t)\n",
    "        enc_std_t = self.enc_std(enc_t)\n",
    "        return enc_mean_t , enc_std_t\n",
    "        \n",
    "    \n",
    "    def decode (self, phi_z_t , h):\n",
    "        phi_z_t = self._concatenate (phi_z_t , h)\n",
    "        dec_t = self.dec(phi_z_t)\n",
    "        dec_mean_t = self.dec_mean(dec_t)\n",
    "        dec_std_t = self.dec_std(dec_t)\n",
    "        return dec_mean_t , dec_std_t\n",
    "        \n",
    "    def forward(self, x):\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "\n",
    "        h = Variable(torch.zeros(self.n_layers, x.size(0), self.h_dim))\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            \n",
    "            phi_x_t = self.phi_x(x[:,t,:].float())\n",
    "            #encoder\n",
    "            enc_mean_t , enc_std_t = self.encode(phi_x_t , h)\n",
    "            #prior\n",
    "            prior_t = self.prior(h.reshape (h.shape[1] , n_layers * h_dim))\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "            #sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "            #decoder\n",
    "            dec_mean_t , dec_std_t = self.decode(phi_z_t , h)\n",
    "            \n",
    "            #recurrence\n",
    "            temp = self._concatenate ( torch.cat ( [ phi_x_t , phi_z_t ] , 1) , h )\n",
    "            _, h = self.rnn(temp.reshape(temp.shape[0] , 1 , temp.shape[1]))\n",
    "\n",
    "            #computing losses enc_std_t.mul(0.5).exp_()\n",
    "            #kld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            kld_loss += self._kld_gauss(enc_mean_t, enc_std_t.mul(0.5).exp_(), prior_mean_t, prior_std_t.mul(0.5).exp_())\n",
    "            nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[:,t,:])\n",
    "                \n",
    "            all_enc_std.append(enc_std_t)\n",
    "            all_enc_mean.append(enc_mean_t)\n",
    "            all_dec_mean.append(dec_mean_t)\n",
    "            all_dec_std.append(dec_std_t)\n",
    "\n",
    "\n",
    "        return kld_loss, nll_loss,(all_enc_mean, all_enc_std),(all_dec_mean, all_dec_std)\n",
    "    \n",
    "    def _reparameterized_sample(self, mean, logvar):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "        \"\"\"Using std to compute KLD\"\"\"\n",
    "        kld_element =  (2 * torch.log(std_2) - 2 * torch.log(std_1) +\n",
    "            (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "            std_2.pow(2) - 1)\n",
    "        return 0.5 * torch.sum(kld_element)\n",
    "\n",
    "    def _nll_gauss(self, mean, logvar , x):\n",
    "        a_i = (x-mean)**2 / (2 *  torch.exp(logvar))\n",
    "        b_i = 0.5 * logvar\n",
    "        c_i = 0.5 * np.log (2 * np.pi)\n",
    "        loss = torch.sum( a_i + b_i +c_i)\n",
    "        return loss\n",
    "    \n",
    "    def _concatenate (self , a , b) :\n",
    "        for i in range(len(b)):\n",
    "            a = torch.cat([a , b[i]] , 1)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Hyper-Parameters and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "x_dim = 25\n",
    "h_dim = 50\n",
    "z_dim = 2\n",
    "n_layers =  2\n",
    "n_epochs = 5\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "seed = 100\n",
    "print_every = 10\n",
    "save_every = 10\n",
    "#manual seed\n",
    "torch.manual_seed(seed)\n",
    "#init model + optimizer + datasets\n",
    "train_loader = torch.utils.data.DataLoader ( dataset = train_resampled ,  batch_size = 100 , shuffle= True)\n",
    "test_loader = torch.utils.data.DataLoader (  dataset = test_resampled , shuffle= True)\n",
    "model = VRNN(x_dim, h_dim, z_dim, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    epoch_loss = np.zeros(int(len (train_resampled) / batch_size ))\n",
    "    epoch_div = np.zeros(int(len (train_resampled) / batch_size))\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        data = Variable(data)\n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        kld_loss, nll_loss, _, _ = model(data)\n",
    "        epoch_loss [batch_idx] = nll_loss\n",
    "        epoch_div [batch_idx] = kld_loss\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                kld_loss.data / batch_size,\n",
    "                nll_loss.data / batch_size))\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss += loss.data\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    return epoch_loss, epoch_div\n",
    "    \n",
    "def test(epoch):\n",
    "    \"\"\"uses test data to evaluate \n",
    "    likelihood of the model\"\"\"\n",
    "    mean_kld_loss, mean_nll_loss = 0, 0\n",
    "    epoch_loss = np.zeros(len(test_resampled))\n",
    "    epoch_div = np.zeros(len(test_resampled))\n",
    "    for i, (data) in enumerate(test_loader):                                           \n",
    "        \n",
    "        data = Variable(data.reshape(1,47,25))\n",
    "        kld_loss, nll_loss, _, _ = model(data)\n",
    "        epoch_div [i] = kld_loss\n",
    "        epoch_loss [i] = nll_loss\n",
    "        mean_kld_loss += kld_loss.data\n",
    "        mean_nll_loss += nll_loss.data\n",
    "\n",
    "    mean_kld_loss /= len(test_loader.dataset)\n",
    "    mean_nll_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "        mean_kld_loss, mean_nll_loss))\n",
    "    return epoch_loss, epoch_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Training and Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/13400 (0%)]\t KLD Loss: 0.163556 \t NLL Loss: 1737.250330\n",
      "Train Epoch: 1 [1000/13400 (7%)]\t KLD Loss: 2.725532 \t NLL Loss: 1532.871011\n",
      "Train Epoch: 1 [2000/13400 (15%)]\t KLD Loss: 0.665980 \t NLL Loss: 1325.652647\n",
      "Train Epoch: 1 [3000/13400 (22%)]\t KLD Loss: 0.433970 \t NLL Loss: 1219.877583\n",
      "Train Epoch: 1 [4000/13400 (30%)]\t KLD Loss: 0.430631 \t NLL Loss: 1158.016110\n",
      "Train Epoch: 1 [5000/13400 (37%)]\t KLD Loss: 0.356759 \t NLL Loss: 1124.760518\n",
      "Train Epoch: 1 [6000/13400 (45%)]\t KLD Loss: 0.202558 \t NLL Loss: 1107.591238\n",
      "Train Epoch: 1 [7000/13400 (52%)]\t KLD Loss: 0.063988 \t NLL Loss: 1098.332810\n",
      "Train Epoch: 1 [8000/13400 (60%)]\t KLD Loss: 0.032896 \t NLL Loss: 1093.416665\n",
      "Train Epoch: 1 [9000/13400 (67%)]\t KLD Loss: 0.015872 \t NLL Loss: 1090.735062\n",
      "Train Epoch: 1 [10000/13400 (75%)]\t KLD Loss: 0.001007 \t NLL Loss: 1089.294129\n",
      "Train Epoch: 1 [11000/13400 (82%)]\t KLD Loss: 0.002384 \t NLL Loss: 1088.433236\n",
      "Train Epoch: 1 [12000/13400 (90%)]\t KLD Loss: 0.000204 \t NLL Loss: 1087.868525\n",
      "Train Epoch: 1 [13000/13400 (97%)]\t KLD Loss: 0.002009 \t NLL Loss: 1087.536966\n",
      "====> Epoch: 1 Average loss: 1185.7948\n",
      "====> Test set loss: KLD Loss = 0.0048, NLL Loss = 1087.3783 \n",
      "Train Epoch: 2 [0/13400 (0%)]\t KLD Loss: 0.001916 \t NLL Loss: 1087.372011\n",
      "Train Epoch: 2 [1000/13400 (7%)]\t KLD Loss: 0.022287 \t NLL Loss: 1083.873140\n",
      "Train Epoch: 2 [2000/13400 (15%)]\t KLD Loss: 0.046819 \t NLL Loss: 1086.897257\n",
      "Train Epoch: 2 [3000/13400 (22%)]\t KLD Loss: 0.102371 \t NLL Loss: 1081.540628\n",
      "Train Epoch: 2 [4000/13400 (30%)]\t KLD Loss: 0.204130 \t NLL Loss: 1081.834183\n",
      "Train Epoch: 2 [5000/13400 (37%)]\t KLD Loss: 0.072810 \t NLL Loss: 1081.328310\n",
      "Train Epoch: 2 [6000/13400 (45%)]\t KLD Loss: 0.011731 \t NLL Loss: 1081.357114\n",
      "Train Epoch: 2 [7000/13400 (52%)]\t KLD Loss: 0.006916 \t NLL Loss: 1081.496847\n",
      "Train Epoch: 2 [8000/13400 (60%)]\t KLD Loss: 0.003918 \t NLL Loss: 1080.566056\n",
      "Train Epoch: 2 [9000/13400 (67%)]\t KLD Loss: 0.003242 \t NLL Loss: 1080.478662\n",
      "Train Epoch: 2 [10000/13400 (75%)]\t KLD Loss: 0.002607 \t NLL Loss: 1080.404702\n",
      "Train Epoch: 2 [11000/13400 (82%)]\t KLD Loss: 0.002099 \t NLL Loss: 1080.323340\n",
      "Train Epoch: 2 [12000/13400 (90%)]\t KLD Loss: 0.001226 \t NLL Loss: 1080.268517\n",
      "Train Epoch: 2 [13000/13400 (97%)]\t KLD Loss: 0.000957 \t NLL Loss: 1080.193270\n",
      "====> Epoch: 2 Average loss: 1082.6667\n",
      "====> Test set loss: KLD Loss = 0.0016, NLL Loss = 1080.1752 \n",
      "Train Epoch: 3 [0/13400 (0%)]\t KLD Loss: 0.000861 \t NLL Loss: 1080.190649\n",
      "Train Epoch: 3 [1000/13400 (7%)]\t KLD Loss: 0.000704 \t NLL Loss: 1080.156570\n",
      "Train Epoch: 3 [2000/13400 (15%)]\t KLD Loss: 0.000542 \t NLL Loss: 1080.098188\n",
      "Train Epoch: 3 [3000/13400 (22%)]\t KLD Loss: 0.000486 \t NLL Loss: 1080.092559\n",
      "Train Epoch: 3 [4000/13400 (30%)]\t KLD Loss: 0.000397 \t NLL Loss: 1080.050929\n",
      "Train Epoch: 3 [5000/13400 (37%)]\t KLD Loss: 0.000379 \t NLL Loss: 1080.029561\n",
      "Train Epoch: 3 [6000/13400 (45%)]\t KLD Loss: 0.000366 \t NLL Loss: 1080.013018\n",
      "Train Epoch: 3 [7000/13400 (52%)]\t KLD Loss: 0.000291 \t NLL Loss: 1079.997237\n",
      "Train Epoch: 3 [8000/13400 (60%)]\t KLD Loss: 0.000263 \t NLL Loss: 1079.992773\n",
      "Train Epoch: 3 [9000/13400 (67%)]\t KLD Loss: 0.000246 \t NLL Loss: 1079.969341\n",
      "Train Epoch: 3 [10000/13400 (75%)]\t KLD Loss: 0.000229 \t NLL Loss: 1079.976513\n",
      "Train Epoch: 3 [11000/13400 (82%)]\t KLD Loss: 0.000211 \t NLL Loss: 1079.947112\n",
      "Train Epoch: 3 [12000/13400 (90%)]\t KLD Loss: 0.000203 \t NLL Loss: 1079.937130\n",
      "Train Epoch: 3 [13000/13400 (97%)]\t KLD Loss: 0.000206 \t NLL Loss: 1079.927359\n",
      "====> Epoch: 3 Average loss: 1080.0257\n",
      "====> Test set loss: KLD Loss = 0.0015, NLL Loss = 1079.9295 \n",
      "Train Epoch: 4 [0/13400 (0%)]\t KLD Loss: 0.000184 \t NLL Loss: 1079.924369\n",
      "Train Epoch: 4 [1000/13400 (7%)]\t KLD Loss: 0.000178 \t NLL Loss: 1079.916390\n",
      "Train Epoch: 4 [2000/13400 (15%)]\t KLD Loss: 0.000166 \t NLL Loss: 1079.908219\n",
      "Train Epoch: 4 [3000/13400 (22%)]\t KLD Loss: 0.000166 \t NLL Loss: 1079.901128\n",
      "Train Epoch: 4 [4000/13400 (30%)]\t KLD Loss: 0.000157 \t NLL Loss: 1079.894066\n",
      "Train Epoch: 4 [5000/13400 (37%)]\t KLD Loss: 0.000150 \t NLL Loss: 1079.887506\n",
      "Train Epoch: 4 [6000/13400 (45%)]\t KLD Loss: 0.000141 \t NLL Loss: 1079.881961\n",
      "Train Epoch: 4 [7000/13400 (52%)]\t KLD Loss: 0.000138 \t NLL Loss: 1079.887842\n",
      "Train Epoch: 4 [8000/13400 (60%)]\t KLD Loss: 0.000145 \t NLL Loss: 1079.872229\n",
      "Train Epoch: 4 [9000/13400 (67%)]\t KLD Loss: 0.000120 \t NLL Loss: 1079.866337\n",
      "Train Epoch: 4 [10000/13400 (75%)]\t KLD Loss: 0.000123 \t NLL Loss: 1079.862456\n",
      "Train Epoch: 4 [11000/13400 (82%)]\t KLD Loss: 0.000124 \t NLL Loss: 1079.857691\n",
      "Train Epoch: 4 [12000/13400 (90%)]\t KLD Loss: 0.000112 \t NLL Loss: 1079.853389\n",
      "Train Epoch: 4 [13000/13400 (97%)]\t KLD Loss: 0.000111 \t NLL Loss: 1079.849229\n",
      "====> Epoch: 4 Average loss: 1079.8915\n",
      "====> Test set loss: KLD Loss = 0.0014, NLL Loss = 1079.8534 \n",
      "Train Epoch: 5 [0/13400 (0%)]\t KLD Loss: 0.000122 \t NLL Loss: 1079.848354\n",
      "Train Epoch: 5 [1000/13400 (7%)]\t KLD Loss: 0.000098 \t NLL Loss: 1079.864836\n",
      "Train Epoch: 5 [2000/13400 (15%)]\t KLD Loss: 0.000113 \t NLL Loss: 1079.841954\n",
      "Train Epoch: 5 [3000/13400 (22%)]\t KLD Loss: 0.000094 \t NLL Loss: 1079.839366\n",
      "Train Epoch: 5 [4000/13400 (30%)]\t KLD Loss: 0.000095 \t NLL Loss: 1079.836145\n",
      "Train Epoch: 5 [5000/13400 (37%)]\t KLD Loss: 0.000101 \t NLL Loss: 1079.834147\n",
      "Train Epoch: 5 [6000/13400 (45%)]\t KLD Loss: 0.000092 \t NLL Loss: 1079.830712\n",
      "Train Epoch: 5 [7000/13400 (52%)]\t KLD Loss: 0.000092 \t NLL Loss: 1079.828922\n",
      "Train Epoch: 5 [8000/13400 (60%)]\t KLD Loss: 0.000090 \t NLL Loss: 1079.826391\n",
      "Train Epoch: 5 [9000/13400 (67%)]\t KLD Loss: 0.000087 \t NLL Loss: 1079.825429\n",
      "Train Epoch: 5 [10000/13400 (75%)]\t KLD Loss: 0.000094 \t NLL Loss: 1079.835289\n",
      "Train Epoch: 5 [11000/13400 (82%)]\t KLD Loss: 0.000074 \t NLL Loss: 1079.822369\n",
      "Train Epoch: 5 [12000/13400 (90%)]\t KLD Loss: 0.000080 \t NLL Loss: 1079.819529\n",
      "Train Epoch: 5 [13000/13400 (97%)]\t KLD Loss: 0.000075 \t NLL Loss: 1079.817846\n",
      "====> Epoch: 5 Average loss: 1079.8400\n",
      "====> Test set loss: KLD Loss = 0.0013, NLL Loss = 1079.8220 \n"
     ]
    }
   ],
   "source": [
    "train_error = np.zeros([n_epochs , int(len (train_resampled) / batch_size ) ])\n",
    "train_div = np.zeros([n_epochs , int(len (train_resampled) / batch_size ) ])\n",
    "test_error , test_div  = np.zeros([n_epochs , len(test_resampled)]) , np.zeros([n_epochs , len(test_resampled)]) \n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    tr = train(epoch)\n",
    "    train_error [epoch-1 , :] = tr [0]\n",
    "    train_div [epoch-1 , :] = tr [1] \n",
    "    te = test(epoch)\n",
    "    test_error [epoch-1 , :] = te [0]\n",
    "    test_div [epoch-1 , :] = te [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Training Data, Model, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path +'C_train_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_error, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_test_error.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_error, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_train_div.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_div, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_test_div.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_div, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_Model.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_train_loader.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(path +'C_test_loader.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE on Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for Test Data is: 0.010409119305771385\n",
      "Variance of RMSE for Test Data is: 0.003116244925750811\n"
     ]
    }
   ],
   "source": [
    "path = 'Path to the Data'\n",
    "with open(path, 'rb') as handle:\n",
    "    test_resampled = pickle.load(handle)\n",
    "test_resampled = [test_resampled[idx].values for idx in range(len(test_resampled))]\n",
    "result =  [ model (torch.tensor (test_resampled[idx]).reshape(1,48,25)) for idx in range(len(test_resampled)) ]\n",
    "torch.manual_seed(seed)\n",
    "dist = [ torch.distributions.normal.Normal (result[idx][3][0][0][:7] , result[idx][3][1][0][:7].mul(0.5).exp_()) for idx in range(len(result))]\n",
    "recon_x = [ dist[idx].sample((10000,)).mean(0) for idx in range(len(dist)) ]\n",
    "error = [ _rmse(test_resampled[idx][-1][:7] , np.array(recon_x[idx][-1][:7])) for idx in range(len(test_resampled)) ]\n",
    "print ('Average RMSE for Test Data is: '+str(np.mean(error)))\n",
    "print ('Variance of RMSE for Test Data is: '+str(np.std(error)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
